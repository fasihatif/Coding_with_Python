{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the html content of a website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case bs4 throws error try\n",
    "# !pip install --upgrade html5lib==1.0b8\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('p', class_='outer-text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_='inner-text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('p', class_='inner-text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the elements of the site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since every web page is different and html can get very large and messy, the easiest way to find elements that you are interested in is to start from the browser window. So next we will quickly look at how to find elements using the developer tools in your browser. Open the following webpage in your browser (preferably Chrome): http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579#.Wkwh8VQ-fVo \n",
    "\n",
    "Find the developer tools in your browser. (In Chrome, it's view --> developer --> developer tools or Control+Shift+C on Windows and Command+Shift+C on Mac) You should end up with a panel at the bottom or the right side of the browser like what you see below. Make sure the Elements panel is highlighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579\")\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_=\"myforecast-current-lrg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_=\"myforecast-current-lrg\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.find_all('p', class_=\"myforecast-current-lrg\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_=\"myforecast-current-lrg\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_=\"myforecast-current-sm\")[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using dictionary for making queries and collecting response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_dict = {\n",
    "    'Honolulu':[21.3049, -157.8579],\n",
    "    'Times Square':[40.757339, -73.985992],\n",
    "    'Yosemite':[37.8651011, -119.5383294]\n",
    "}\n",
    "latlon_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = {}\n",
    "for place,coordinates in latlon_dict.items():\n",
    "    url = \"http://forecast.weather.gov/MapClick.php?lat={}&lon={}\".format(\n",
    "        coordinates[0], coordinates[1])\n",
    "    print(place)\n",
    "    print(url)\n",
    "    resp = requests.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    temp_C = soup.find_all('p', class_=\"myforecast-current-sm\")[0].text\n",
    "    response_dict[place] = temp_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place,temperature in response_dict.items():\n",
    "    print(\"The current temperature in {} is {}.\".format(place, temperature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving dictinaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('mydict.npy', response_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dictionary = np.load('mydict.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(read_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - exercise\n",
    "\n",
    "We need the zip codes of the 5 landmarks in our data. Fortunatelly Google shows the zip codes at a fixed place if using the right searchphase. <br>\n",
    "Open this link and using the Inspect tool in the browser try to find the class of the HTML element of a zip code shown at the top of the page! <br> \n",
    "https://www.google.com/search?q=San+Jose+zip+code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - check yourself\n",
    "\n",
    "The zip code is under a div of class \"title\" inside a div of class \"junCMe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - exercise\n",
    "Now use the requests library to get the html content of this page and create a BeautifulSoup object called soup from this content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://www.google.com/search?q=San+Jose+zip+code\")\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - check yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your soup object is correct\n"
     ]
    }
   ],
   "source": [
    "if type(soup) == BeautifulSoup and '94089' in soup.text:\n",
    "    print('Your soup object is correct')\n",
    "else:\n",
    "    print('Your soup object is NOT correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - exercise\n",
    "Try to find all the div elements of class IAznY in your soup object. How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('div', class_= 'IAznY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - check yourself\n",
    "If you haven't found any div of this class you were right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - exercise\n",
    "So it looks like that the scraped HTML code doesn't have the elements you saw in the browser. The reason is that when opening the url in the browser, it uses JavaScript to format the page, but when we scraped it, only the plaine HTML was sent. <br><br>\n",
    "To see the same content in the browser disable JavaScript usage by following this directions:  https://productforums.google.com/forum/#!msg/chrome/BYOQskiuGU0/dO592rlLbJ0J). <br><br>\n",
    "Then open the page again and find using the Inspect tool find the HTML elemnt containg the zip code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - check yourself\n",
    "\n",
    "Each zip code is under a div of class \"BNeawe deIvCb AP7Wnd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - exercise\n",
    "Try to find all the div elements of class \"BNeawe deIvCb AP7Wnd\" in your soup object. How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('div', class_=\"BNeawe deIvCb AP7Wnd\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - check yourself\n",
    "You should find 69 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - exercise\n",
    "Make a list called zipcode_list that contains the text from all the \"BNeawe deIvCb AP7Wnd\" div elements. Keep only those that consist of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zipcode_list = []\n",
    "\n",
    "for x in range(1,len(soup.find_all('div', class_=\"BNeawe deIvCb AP7Wnd\"))):\n",
    "    a = soup.find_all('div', class_=\"BNeawe deIvCb AP7Wnd\")[x].text\n",
    "    if a.isdigit():\n",
    "        zipcode_list.append(a)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - check yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your list is correct\n"
     ]
    }
   ],
   "source": [
    "if sorted(zipcode_list)[0] == '94088' and len(zipcode_list) == 66:\n",
    "    print('Your list is correct')\n",
    "else:\n",
    "    print('Your list is NOT correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - exercise\n",
    "Read in the weather csv into a pandas dataframe called station. <br>\n",
    "Create a dictionary called zipcode_dict which keys are the unique values from the landmark column and the value of each key is an empty list. You print the unique values and create the dictionary by hand or as an advanced task, try to create the dictionary without typing any landmark name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "station = pd.read_csv('weather.csv')\n",
    "zipcode_dict = {}\n",
    "for lmk in station['landmark'].unique():\n",
    "    zipcode_dict[lmk] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - check yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dictionary is correct\n"
     ]
    }
   ],
   "source": [
    "if sorted(list(zipcode_dict.items())) == [('Mountain View', []),\n",
    "                                         ('Palo Alto', []),\n",
    "                                         ('Redwood City', []),\n",
    "                                         ('San Francisco', []),\n",
    "                                         ('San Jose', [])]:\n",
    "    print('Your dictionary is correct')\n",
    "else:\n",
    "    print('Your dictionary is NOT correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - exercise\n",
    "Loop the keys from the zipcode_dict and for each key print the url you would use to search the zip codes of a given city in google by using string formatting. <br>\n",
    "For example if the city is Palo Alto the url should be: <br>\n",
    "https://www.google.com/search?q=Palo Alto zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=San+Francisco+zip+code\n",
      "https://www.google.com/search?q=Palo+Alto+zip+code\n",
      "https://www.google.com/search?q=Mountain+View+zip+code\n",
      "https://www.google.com/search?q=San+Jose+zip+code\n",
      "https://www.google.com/search?q=Redwood+City+zip+code\n"
     ]
    }
   ],
   "source": [
    "### Your code here\n",
    "\n",
    "for city_name in zipcode_dict:\n",
    "    city_name = city_name.replace(\" \",\"+\")\n",
    "    url = 'https://www.google.com/search?q={}+zip+code'.format(city_name)\n",
    "    print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 - exercise\n",
    "Similarly as before, loop the keys from the zipcode_dict and for each key inside the loop:\n",
    "- Get a response object to the url you would use to search the zip codes of a given city in google. <br>\n",
    "- Make a soup from that resopnse object. <br>\n",
    "- Make a list of all zip codes in the soup object. You can find the zip codes as in Exercise 6<br>\n",
    "- Assign this list as value to the key in the zipcode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city_name in zipcode_dict.keys():\n",
    "    zipcode_list = []\n",
    "    city = city_name.replace(\" \",\"+\")\n",
    "    url = 'https://www.google.com/search?q={}+zip+code'.format(city)\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    \n",
    "    for x in range(1,len(soup.find_all('div', class_=\"BNeawe deIvCb AP7Wnd\"))):\n",
    "        a = soup.find_all('div', class_=\"BNeawe deIvCb AP7Wnd\")[x].text\n",
    "        if a.isdigit():\n",
    "            zipcode_list.append(a)\n",
    "            zipcode_dict[city_name] = zipcode_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 - check yourslef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dictionary is correct\n"
     ]
    }
   ],
   "source": [
    "if sorted([len(x) for x in zipcode_dict.values()]) == [6, 7, 10, 56, 66]:\n",
    "    print('Your dictionary is correct')\n",
    "else:\n",
    "    print('Your dictionary is NOT correct')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
